{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import unidecode\n",
    "from utils import preprocessText, removeStopwords, lowerToken\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from numpy import array, asarray, zeros\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\msego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\msego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First we need to download nltk stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exijo un corto de Bucky y Sam viendo Rogers: T...</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ya viendo lo mas nuevo de @MarvelLATAM \\r\\n#Ha...</td>\n",
       "      <td>Positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hasta el momento #Hawkeye la mejor serie de @M...</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#TheEternals es la pelicula de @MarvelLATAM me...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@MarvelLATAM @disneyplusla #Hawkeye  Capitulo ...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  Exijo un corto de Bucky y Sam viendo Rogers: T...    Neutro\n",
       "1  Ya viendo lo mas nuevo de @MarvelLATAM \\r\\n#Ha...  Positivo\n",
       "2  Hasta el momento #Hawkeye la mejor serie de @M...    Neutro\n",
       "3  #TheEternals es la pelicula de @MarvelLATAM me...  Negativo\n",
       "4  @MarvelLATAM @disneyplusla #Hawkeye  Capitulo ...  Negativo"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"corpus/dataExample_0.csv\", names=['text','sentiment'], header=0)\n",
    "data2 = pd.read_csv(\"corpus/dataExample_1.csv\", names=['text','sentiment'], header=0)\n",
    "data3 = pd.read_csv(\"corpus/dataExample_2.csv\", names=['text','sentiment'], header=0)\n",
    "#Join all dataframes in a single dataframe\n",
    "data = pd.concat([data,data3,data2])\n",
    "\n",
    "#Check the data is correctly readed\n",
    "print(\"Dataframe data\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(904, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we check the shape\n",
    "print(\"Dataframe shape\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe sentiment count\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Neutro      415\n",
       "Negativo    316\n",
       "Positivo    173\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we check the sentiment count\n",
    "print(\"Dataframe sentiment count\")\n",
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exijo un corto de Bucky y Sam viendo Rogers: T...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>Exijo un corto de Bucky Sam viendo Rogers The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ya viendo lo mas nuevo de @MarvelLATAM \\r\\n#Ha...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>Ya viendo lo mas nuevo de Hawkeye Revolucionan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hasta el momento #Hawkeye la mejor serie de @M...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>Hasta el momento Hawkeye la mejor serie de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#TheEternals es la pelicula de @MarvelLATAM me...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>TheEternals es la pelicula de menos Marvel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@MarvelLATAM @disneyplusla #Hawkeye  Capitulo ...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>Hawkeye Capitulo Nada rescatable Peleas sin f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment  \\\n",
       "0  Exijo un corto de Bucky y Sam viendo Rogers: T...    Neutro   \n",
       "1  Ya viendo lo mas nuevo de @MarvelLATAM \\r\\n#Ha...  Positivo   \n",
       "2  Hasta el momento #Hawkeye la mejor serie de @M...    Neutro   \n",
       "3  #TheEternals es la pelicula de @MarvelLATAM me...  Negativo   \n",
       "4  @MarvelLATAM @disneyplusla #Hawkeye  Capitulo ...  Negativo   \n",
       "\n",
       "                                          text_clean  \n",
       "0  Exijo un corto de Bucky Sam viendo Rogers The ...  \n",
       "1  Ya viendo lo mas nuevo de Hawkeye Revolucionan...  \n",
       "2         Hasta el momento Hawkeye la mejor serie de  \n",
       "3         TheEternals es la pelicula de menos Marvel  \n",
       "4   Hawkeye Capitulo Nada rescatable Peleas sin f...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pre process tweets and save them as a new column in the dataframe\n",
    "data['text_clean'] = data['text'].apply(lambda x: preprocessText(str(x)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform sentences into tokens\n",
    "tokens = [word_tokenize(sen) for sen in data.text_clean]\n",
    "\n",
    "#Put all the words to lowercase\n",
    "lower_tokens = [lowerToken(token) for token in tokens]\n",
    "\n",
    "#Import spanish stopwords\n",
    "stoplist = stopwords.words('spanish')\n",
    "\n",
    "#Remove stopwords from sentences for better process\n",
    "filtered_words = [removeStopwords(sen, stoplist) for sen in lower_tokens]\n",
    "\n",
    "#Update processed text from dataframe with the new filtered sentences\n",
    "data['text_clean'] = [' '.join(sen) for sen in filtered_words]\n",
    "#Create a new column that will have the same words but as tokens\n",
    "data['tokens'] = filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform sentiment label to three columns in dataset for three outputs\n",
    "pos = []\n",
    "neg = []\n",
    "neu = []\n",
    "\n",
    "for sent in data.sentiment:\n",
    "    if sent == 'P' or sent=='pos' or sent=='positivo':\n",
    "        neu.append(0)\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "    elif sent == 'N' or sent=='neg' or sent=='negativo':\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "        neu.append(0)\n",
    "    else:\n",
    "        neu.append(1)\n",
    "        pos.append(0)\n",
    "        neg.append(0)\n",
    "        \n",
    "data['Pos'] = pos\n",
    "data['Neg'] = neg\n",
    "data['Neu'] = neu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neu</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>exijo corto bucky sam viendo rogers the musica...</td>\n",
       "      <td>[exijo, corto, bucky, sam, viendo, rogers, the...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>viendo mas nuevo hawkeye revolucionando genero...</td>\n",
       "      <td>[viendo, mas, nuevo, hawkeye, revolucionando, ...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>momento hawkeye mejor serie</td>\n",
       "      <td>[momento, hawkeye, mejor, serie]</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>theeternals pelicula menos marvel</td>\n",
       "      <td>[theeternals, pelicula, menos, marvel]</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hawkeye capitulo rescatable peleas fuerza chic...</td>\n",
       "      <td>[hawkeye, capitulo, rescatable, peleas, fuerza...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_clean  \\\n",
       "0  exijo corto bucky sam viendo rogers the musica...   \n",
       "1  viendo mas nuevo hawkeye revolucionando genero...   \n",
       "2                        momento hawkeye mejor serie   \n",
       "3                  theeternals pelicula menos marvel   \n",
       "4  hawkeye capitulo rescatable peleas fuerza chic...   \n",
       "\n",
       "                                              tokens sentiment  Pos  Neu  Neg  \n",
       "0  [exijo, corto, bucky, sam, viendo, rogers, the...    Neutro    0    1    0  \n",
       "1  [viendo, mas, nuevo, hawkeye, revolucionando, ...  Positivo    0    1    0  \n",
       "2                   [momento, hawkeye, mejor, serie]    Neutro    0    1    0  \n",
       "3             [theeternals, pelicula, menos, marvel]  Negativo    0    1    0  \n",
       "4  [hawkeye, capitulo, rescatable, peleas, fuerza...  Negativo    0    1    0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Redeclare dataframe with selected columns\n",
    "data = data[['text_clean', 'tokens', 'sentiment', 'Pos', 'Neu', 'Neg']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data for test and training \n",
    "data_train, data_test = train_test_split(data, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5820 words, with a vocabulary size of 2699\n",
      "Max sentence length is 29\n"
     ]
    }
   ],
   "source": [
    "#Get total words in the train dataframe\n",
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "\n",
    "#Get all the sentence lengths from train dataframe\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "\n",
    "#Get all the words without duplicates in the train dataframe\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "\n",
    "print(\"%s words, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737 words, with a vocabulary size of 521\n",
      "Max sentence length is 32\n"
     ]
    }
   ],
   "source": [
    "#Get total words in the test dataframe\n",
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "\n",
    "#Get all the sentence lengths from test dataframe\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "\n",
    "#Get all the words without duplicates in the test dataframe\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "\n",
    "print(\"%s words, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2699 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "#Now we start using tokenizer for sentences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 50  #Max length that a sentence should have\n",
    "EMBEDDING_DIM = 300   #Dimension of embedding (the same as the dimension of glove embeddings)\n",
    "\n",
    "#Declare Tokenizer\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "\n",
    "#Fit tokenizer with training data\n",
    "tokenizer.fit_on_texts(data_train[\"text_clean\"].tolist())\n",
    "\n",
    "#Transform sentences from both datasets into sequences with tokenizer\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"text_clean\"].tolist())\n",
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"text_clean\"].tolist())\n",
    "\n",
    "#Pad the sequences adding 0s to reach the max sequence length\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2700, 300)\n"
     ]
    }
   ],
   "source": [
    "#load glove embeddings\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "#Open glove file\n",
    "glove_file = open('glove/glove-sbwc.i25.vec', encoding=\"utf8\")\n",
    "\n",
    "#Iterate all lines in glove file \n",
    "for line in glove_file:\n",
    "    #Split words\n",
    "    records = line.split()\n",
    "    \n",
    "    #The first line should not be considered\n",
    "    if len(records) == 2:\n",
    "        continue\n",
    "        \n",
    "    #Save data in the dictionary\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "\n",
    "#Close glove file\n",
    "glove_file.close()\n",
    "\n",
    "#Create an array with the glove dimension of the embeddings and total unique tokens\n",
    "train_embedding_weights = zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "\n",
    "#Save embedding weights using weights from glove if has the word, otherwise use a random array with the same dimension\n",
    "for word, index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = embeddings_dictionary[word] if word in embeddings_dictionary else np.random.rand(EMBEDDING_DIM)\n",
    "\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we define the model\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    #Create the embedding layer of the model\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    #Create the model as a sequential model using keras\n",
    "    model = Sequential()\n",
    "    #Add the embedding layer to the model\n",
    "    model.add(embedding_layer)\n",
    "    #Add a convolutional layer of one dimension and 512 filters with tanh activation\n",
    "    model.add(Conv1D(512, 10, activation='tanh'))\n",
    "    #Add a global pooling layer\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    #Add a dense layer with 3 outputs using softmax activation, as we have 3 possible answers\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    #Compile model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the label names of the answers\n",
    "label_names = ['Pos', 'Neu', 'Neg']\n",
    "#Set training data to fit model\n",
    "y_train = data_train[label_names].values\n",
    "x_train = train_cnn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 300)           810000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 41, 512)           1536512   \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 512)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,348,051\n",
      "Trainable params: 1,538,051\n",
      "Non-trainable params: 810,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "7/7 [==============================] - 3s 327ms/step - loss: 0.2973 - acc: 0.8376 - val_loss: 0.1615 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 2s 283ms/step - loss: 0.0562 - acc: 1.0000 - val_loss: 0.1079 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 2s 301ms/step - loss: 0.0295 - acc: 1.0000 - val_loss: 0.0873 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 2s 308ms/step - loss: 0.0200 - acc: 1.0000 - val_loss: 0.0767 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 2s 297ms/step - loss: 0.0152 - acc: 1.0000 - val_loss: 0.0705 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#Set number of epochs and batch size\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "#Set test data to evaluate model\n",
    "y_test = data_test[label_names].values\n",
    "X_test = test_cnn_data\n",
    "\n",
    "#Create model\n",
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(label_names)))\n",
    "\n",
    "#Fit model and evaluate\n",
    "hist = model.fit(x_train, y_train, epochs=num_epochs, validation_data=(X_test, y_test), shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "['', 's', 't', 'u', 'v', '', '', 'c', 'h', '', 'v', '', '', '', '', '', '', '', 'p', '', '', '', 'c', 'u', '', '', '', 'd', '', '', '', 't', '', '', 'n', '', '', 's', '', '', '', '', 's', 'p', 'i', 'd', '', '', 'v', '', '', 's', '', '', 'c', '', 'n', '', 'i', '', '', '', 'd', '']\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.01593, 0.96941, 0.01466]\n",
      "[0.01464, 0.97023, 0.01513]\n",
      "[0.22705, 0.57324, 0.19972]\n",
      "[0.08233, 0.83944, 0.07822]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.31734, 0.38315, 0.29951]\n",
      "[0.26002, 0.46827, 0.27171]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.08233, 0.83944, 0.07822]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.11508, 0.74249, 0.14244]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.31734, 0.38315, 0.29951]\n",
      "[0.22705, 0.57324, 0.19972]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.15605, 0.64048, 0.20347]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.01464, 0.97023, 0.01513]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.10814, 0.78367, 0.10819]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.01593, 0.96941, 0.01466]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.01593, 0.96941, 0.01466]\n",
      "[0.11508, 0.74249, 0.14244]\n",
      "[0.03829, 0.92933, 0.03238]\n",
      "[0.15605, 0.64048, 0.20347]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.08233, 0.83944, 0.07822]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.01593, 0.96941, 0.01466]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.31734, 0.38315, 0.29951]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.10814, 0.78367, 0.10819]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.03829, 0.92933, 0.03238]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.26172, 0.47646, 0.26182]\n",
      "[0.15605, 0.64048, 0.20347]\n",
      "[0.26172, 0.47646, 0.26182]\n"
     ]
    }
   ],
   "source": [
    "#Test the model with example\n",
    "text = \"Estuvo chevere la película de Eternals 🙌, Spiderverse confirmado\"\n",
    "text = [preprocessText(t) for t in text]\n",
    "text = tokenizer.texts_to_sequences(text)\n",
    "print('----------------')\n",
    "print(tokenizer.sequences_to_texts(text))\n",
    "text = pad_sequences(text, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "predictions = model.predict(text)\n",
    "for p in predictions:\n",
    "    p = [round(num,5) for num in p]\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export model\n",
    "import pickle\n",
    "\n",
    "with open('tokenizer3.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_model3\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"cnn_model_3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
